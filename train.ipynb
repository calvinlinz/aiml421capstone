{
    "cells": [{
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [{
                "name": "stdout",
                "output_type": "stream",
                "text": [
                    "Loaded 1492 images from cherry class.\n",
                    "Loaded 1494 images from strawberry class.\n",
                    "Loaded 1494 images from tomato class.\n"
                ]
            }],
            "source": [
                "import os\n",
                "from PIL import Image\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from torchvision import transforms\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "\n",
                "# Define the classes\n",
                "classes = ['cherry', 'strawberry', 'tomato']\n",
                "data_dir = './train_data'\n",
                "\n",
                "# Dictionary to store the loaded images\n",
                "data = {}\n",
                "\n",
                "# List of images to exclude\n",
                "excluded_images = {\n",
                "    'cherry_0055.jpg',\n",
                "    'cherry_0105.jpg',\n",
                "    'cherry_0147.jpg',\n",
                "    'strawberry_0931.jpg',\n",
                "    'tomato_0087.jpg'\n",
                "}\n",
                "\n",
                "for class_name in classes:\n",
                "    class_dir = os.path.join(data_dir, class_name)\n",
                "    images = []\n",
                "    \n",
                "    # Loop through all files in the class directory\n",
                "    for file_name in os.listdir(class_dir):\n",
                "        if file_name.endswith('.jpg'):  # Check for image files\n",
                "            # Check if the file should be excluded\n",
                "            if file_name in excluded_images:\n",
                "                continue  # Skip this file\n",
                "            file_path = os.path.join(class_dir, file_name)\n",
                "            \n",
                "            # Open the image and append it to the list\n",
                "            img = Image.open(file_path)\n",
                "            images.append(img)\n",
                "    \n",
                "    # Store images for this class\n",
                "    data[class_name] = images\n",
                "\n",
                "\n",
                "# Example: Accessing images from the 'cherry' class\n",
                "print(f'Loaded {len(data[\"cherry\"])} images from cherry class.')\n",
                "print(f'Loaded {len(data[\"strawberry\"])} images from strawberry class.')\n",
                "print(f'Loaded {len(data[\"tomato\"])} images from tomato class.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import random\n",
                "\n",
                "\n",
                "def set_seed(seed):\n",
                "    random.seed(seed)  # For Python random\n",
                "    np.random.seed(seed)  # For NumPy random\n",
                "    torch.manual_seed(seed)  # For PyTorch CPU random\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)  # For PyTorch GPU random\n",
                "        torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n",
                "    torch.backends.cudnn.deterministic = True  # Make sure CUDA computations are deterministic\n",
                "    torch.backends.cudnn.benchmark = False  # Disable benchmark mode to make it reproducible\n",
                "\n",
                "# Set a seed value, e.g., 42\n",
                "set_seed(42)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [{
                "name": "stdout",
                "output_type": "stream",
                "text": [
                    "Filtered 1475 images from cherry class.\n",
                    "Filtered 1477 images from strawberry class.\n",
                    "Filtered 1476 images from tomato class.\n",
                    "Removed 52 images in total.\n",
                    "Filtered 4428 images in total.\n"
                ]
            }],
            "source": [
                "# Define the target resolution\n",
                "target_size = (300, 300)\n",
                "\n",
                "# Dictionary to hold filtered data\n",
                "filtered_data = {}\n",
                "\n",
                "count = 0\n",
                "# Iterate through the classes\n",
                "for class_name, images in data.items():\n",
                "    filtered_images = []\n",
                "    \n",
                "    # Check each image for its resolution\n",
                "    for img in images:\n",
                "        if img.size == target_size:\n",
                "            filtered_images.append(img)  # Keep images that match 300x300\n",
                "        else:\n",
                "            count += 1\n",
                "    \n",
                "    # Store only the filtered images in the new dictionary\n",
                "    filtered_data[class_name] = filtered_images\n",
                "\n",
                "# Example: Accessing filtered images\n",
                "print(f'Filtered {len(filtered_data[\"cherry\"])} images from cherry class.')\n",
                "print(f'Filtered {len(filtered_data[\"strawberry\"])} images from strawberry class.')\n",
                "print(f'Filtered {len(filtered_data[\"tomato\"])} images from tomato class.')\n",
                "print(f'Removed {count} images in total.')\n",
                "print(f'Filtered {len(filtered_data[\"cherry\"])+len(filtered_data[\"strawberry\"])+len(filtered_data[\"tomato\"])} images in total.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [{
                "name": "stdout",
                "output_type": "stream",
                "text": [
                    "Input images: 4428\n",
                    "Processed images: 4427\n",
                    "Removed Grayscale images: 1\n",
                    "RGB images: 4426\n",
                    "Outliers: 144\n",
                    "Images in filtered_data: 4283\n",
                    "\n",
                    "Found 144 potential RGB channel-based outliers out of 4427 total images.\n",
                    "Filtered data contains 4283 images after RGB channel-based filtering.\n"
                ]
            }],
            "source": [
                "import numpy as np\n",
                "from collections import defaultdict\n",
                "\n",
                "def detect_and_filter_rgb_outliers(image_data, thresholds):\n",
                "    filtered_data = defaultdict(list)\n",
                "    outliers = []\n",
                "    grayscale_count = 0\n",
                "    total_input_images = sum(len(images) for images in image_data.values())\n",
                "    \n",
                "    for class_name, images in image_data.items():\n",
                "        for img in images:\n",
                "            img_np = np.array(img)  # Convert image to NumPy array\n",
                "            \n",
                "            if len(img_np.shape) == 2:  # Grayscale image (only height and width)\n",
                "                grayscale_count += 1\n",
                "                continue\n",
                "            \n",
                "            # Calculate the mean pixel intensity for each RGB channel\n",
                "            mean_channels = np.mean(img_np, axis=(0, 1))\n",
                "            \n",
                "            # Detect if any of the channels are outside their specific thresholds\n",
                "            condition = (mean_channels < [t[0] for t in thresholds]) | (mean_channels > [t[1] for t in thresholds])\n",
                "            if np.any(condition):\n",
                "                outliers.append(img)\n",
                "            else:\n",
                "                filtered_data[class_name].append(img)\n",
                "    \n",
                "    total_processed_images = sum(len(images) for images in filtered_data.values()) + len(outliers)\n",
                "    \n",
                "    print(f\"Input images: {total_input_images}\")\n",
                "    print(f\"Processed images: {total_processed_images}\")\n",
                "    print(f\"Removed Grayscale images: {grayscale_count}\")\n",
                "    print(f\"RGB images: {total_processed_images - grayscale_count}\")\n",
                "    print(f\"Outliers: {len(outliers)}\")\n",
                "    print(f\"Images in filtered_data: {sum(len(images) for images in filtered_data.values())}\")\n",
                "    \n",
                "    return dict(filtered_data), outliers\n",
                "\n",
                "# Define channel-specific thresholds based on the distributions\n",
                "thresholds = [\n",
                "    (27, 238),  # Red channel (low, high)\n",
                "    (14, 220),  # Green channel (low, high)\n",
                "    (8, 218)    # Blue channel (low, high)\n",
                "]\n",
                "\n",
                "# Use the optimized function with new thresholds\n",
                "filtered_data, rgb_outliers = detect_and_filter_rgb_outliers(filtered_data, thresholds)\n",
                "print(f'\\nFound {len(rgb_outliers)} potential RGB channel-based outliers out of {sum(len(images) for images in filtered_data.values()) + len(rgb_outliers)} total images.')\n",
                "print(f'Filtered data contains {sum(len(images) for images in filtered_data.values())} images after RGB channel-based filtering.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [{
                "name": "stdout",
                "output_type": "stream",
                "text": [
                    "Training X shape: torch.Size([2998, 3, 300, 300]), Training y shape: torch.Size([2998])\n",
                    "Testing X shape: torch.Size([1285, 3, 300, 300]), Testing y shape: torch.Size([1285])\n"
                ]
            }],
            "source": [
                "def normalize(data):\n",
                "    transform = transforms.Compose([\n",
                "        transforms.ToTensor(),  # Converts image to tensor and normalizes to [0, 1]\n",
                "    ])\n",
                "    X_data = []\n",
                "    y_labels = []\n",
                "\n",
                "    label_mapping = {\n",
                "        'cherry': 0,\n",
                "        'strawberry': 1,\n",
                "        'tomato': 2\n",
                "    }\n",
                "\n",
                "    # Step 1: Transform images directly without intermediate NumPy conversion\n",
                "    for label, images in data.items():\n",
                "        for img in images:\n",
                "            img_transformed = transform(img)  # Apply transformation to normalize and convert to tensor\n",
                "            X_data.append(img_transformed)\n",
                "            y_labels.append(label_mapping[label])\n",
                "\n",
                "    # Step 2: Stack tensors together\n",
                "    X = torch.stack(X_data)  # Now, X will be of shape [num_images, 3, 300, 300]\n",
                "    y = torch.tensor(y_labels)\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "    return X_train, X_test, y_train, y_test\n",
                "\n",
                "def get_dataloaders(batch_size):\n",
                "    X_train, X_test, y_train, y_test = normalize(filtered_data)\n",
                "    # Step 4: Create TensorDatasets and DataLoaders\n",
                "    train_dataset = TensorDataset(X_train, y_train)\n",
                "    test_dataset = TensorDataset(X_test, y_test)\n",
                "\n",
                "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
                "    # Check shapes\n",
                "    print(f\"Training X shape: {X_train.shape}, Training y shape: {y_train.shape}\")\n",
                "    print(f\"Testing X shape: {X_test.shape}, Testing y shape: {y_test.shape}\")\n",
                "    return train_loader, test_loader\n",
                "\n",
                "\n",
                "batch_size = 64\n",
                "train_loader, test_loader = get_dataloaders(batch_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "class MLP(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        super(MLP, self).__init__()\n",
                "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
                "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
                "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
                "        self.relu = nn.GELU()\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.relu(self.fc1(x))\n",
                "        x = self.relu(self.fc2(x))\n",
                "        x = self.fc3(x)\n",
                "        return x\n",
                "\n",
                "\n",
                "\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self):  # Use dropout_rate instead of decay for clarity\n",
                "        super(CNN, self,).__init__()\n",
                "\n",
                "        # First convolutional layer: input channels=3, output channels=16, kernel size=3x3\n",
                "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
                "\n",
                "        # Max pooling layer to downsample\n",
                "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "\n",
                "        # Fully connected layers\n",
                "        self.fc1 = nn.Linear(32 * 75 * 75, 128)  # Adjusting for 300x300 input size after pooling\n",
                "        self.fc2 = nn.Linear(128, 3)  # Output size matches the number of classes (cherry, strawberry, tomato)\n",
                "\n",
                "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        self.dropout = nn.Dropout(p=0.04380980725546357)\n",
                "\n",
                "        # Activation function\n",
                "        self.relu = nn.ReLU()\n",
                "\n",
                "    def forward(self, x):\n",
                "        # Apply first conv layer, activation, and pooling\n",
                "        x = self.pool(self.relu(self.conv1(x)))\n",
                "\n",
                "        # Apply second conv layer, activation, and pooling\n",
                "        x = self.pool(self.relu(self.conv2(x)))\n",
                "\n",
                "        # Flatten the output from convolutional layers\n",
                "        x = x.view(-1, 32 * 75 * 75)\n",
                "\n",
                "        # Apply first fully connected layer with dropout\n",
                "        x = self.relu(self.fc1(x))\n",
                "        x = self.dropout(x)  # Apply dropout here\n",
                "\n",
                "        # Apply second fully connected layer\n",
                "        x = self.fc2(x)\n",
                "\n",
                "        return x\n",
                "\n",
                "class FocalLoss(nn.Module):\n",
                "    def __init__(self, alpha=1, gamma=2):\n",
                "        super(FocalLoss, self).__init__()\n",
                "        self.alpha = alpha\n",
                "        self.gamma = gamma\n",
                "        self.ce_loss = nn.CrossEntropyLoss()\n",
                "\n",
                "    def forward(self, inputs, targets):\n",
                "        ce_loss = self.ce_loss(inputs, targets)\n",
                "        pt = torch.exp(-ce_loss)\n",
                "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
                "        return focal_loss\n",
                "    \n",
                "\n",
                "class LabelSmoothingLoss(nn.Module):\n",
                "    def __init__(self, smoothing=0.1, num_classes=3):\n",
                "        \"\"\"\n",
                "        Label smoothing loss for multi-class classification\n",
                "        \n",
                "        Args:\n",
                "            smoothing (float): smoothing factor, typically between 0.0 and 0.2\n",
                "            num_classes (int): number of classes in your dataset\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.smoothing = smoothing\n",
                "        self.num_classes = num_classes\n",
                "        self.confidence = 1.0 - smoothing\n",
                "        \n",
                "    def forward(self, pred, target):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            pred: predictions from model (N, C)\n",
                "            target: target labels (N,)\n",
                "        \"\"\"\n",
                "        pred = F.log_softmax(pred, dim=-1)\n",
                "        \n",
                "        # Create smoothed labels\n",
                "        with torch.no_grad():\n",
                "            true_dist = torch.zeros_like(pred)\n",
                "            true_dist.fill_(self.smoothing / (self.num_classes - 1))\n",
                "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
                "        \n",
                "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
                "from sklearn.model_selection import KFold\n",
                "import numpy as np\n",
                "import optuna\n",
                "import optuna.visualization as vis\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "\n",
                "def train_and_evaluate(train_loader, val_loader, model, optimizer, loss_function, device, num_epochs):\n",
                "    for epoch in range(num_epochs):\n",
                "        model.train()  # Set the model to training mode\n",
                "        total, correct = 0, 0\n",
                "\n",
                "        for inputs, labels in train_loader:\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "\n",
                "            optimizer.zero_grad()  # Zero the gradients\n",
                "            outputs = model(inputs)  # Forward pass\n",
                "            loss = loss_function(outputs, labels)  # Compute the loss\n",
                "            loss.backward()  # Backpropagation\n",
                "            optimizer.step()  # Update the weights\n",
                "\n",
                "            # Calculate accuracy\n",
                "            _, predicted = torch.max(outputs, 1)  # Get predicted class indices\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
                "\n",
                "        train_acc = 100 * correct / total\n",
                "        # print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Accuracy: {train_acc:.2f}%\")\n",
                "\n",
                "        # Validation\n",
                "        model.eval()  # Set the model to evaluation mode\n",
                "        total, correct = 0, 0\n",
                "\n",
                "        with torch.no_grad():  # No gradients needed during validation\n",
                "            for inputs, labels in val_loader:\n",
                "                inputs, labels = inputs.to(device), labels.to(device)\n",
                "                outputs = model(inputs)\n",
                "                \n",
                "                # Calculate accuracy\n",
                "                _, predicted = torch.max(outputs, 1)  # Get predicted class indices\n",
                "                total += labels.size(0)\n",
                "                correct += (predicted == labels).sum().item()  # Count correct predictions\n",
                "\n",
                "        val_acc = 100 * correct / total\n",
                "        # print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {val_acc:.2f}%\")\n",
                "\n",
                "    return val_acc\n",
                "\n",
                "def k_fold(dataset, k_folds, model, optimizer,loss_function, trial=None):\n",
                "    indices = list(range(len(dataset)))\n",
                "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
                "    fold_accuracies = []\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "    for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
                "        train_sampler = SubsetRandomSampler(train_idx)\n",
                "        val_sampler = SubsetRandomSampler(val_idx)\n",
                "        \n",
                "        train_fold_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
                "        val_fold_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
                "\n",
                "        val_accuracy = train_and_evaluate(\n",
                "                train_fold_loader, val_fold_loader,\n",
                "                model, optimizer, loss_function,\n",
                "                device, num_epochs=8\n",
                "            )\n",
                "        fold_accuracies.append(val_accuracy)\n",
                "        if trial != None:\n",
                "            trial.report(val_accuracy, fold)\n",
                "            if trial.should_prune():\n",
                "                raise optuna.TrialPruned()\n",
                "    \n",
                "    return np.mean(fold_accuracies)\n",
                "\n",
                "def objective(trial):\n",
                "    kernel_size = trial.suggest_categorical(\"kernel_size\", [2,3,4])\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    k_folds = 5\n",
                "    dataset = train_loader.dataset\n",
                "    model = CNN().to(device)\n",
                "    optimizer = optim.Adam(model.parameters(), lr=0.00020711569050274695)  \n",
                "    loss_function = nn.CrossEntropyLoss()\n",
                "    return k_fold(dataset,k_folds,model,optimizer, loss_function,trial)\n",
                "\n",
                "\n",
                "def run_optimization(n_trials=10):\n",
                "    study = optuna.create_study(\n",
                "        direction='maximize',\n",
                "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5)\n",
                "    )\n",
                "    study.optimize(\n",
                "        objective,\n",
                "        n_trials=n_trials,\n",
                "        timeout=3600  # 1 hour timeout\n",
                "    )\n",
                "    \n",
                "    print('\\nBest trial:')\n",
                "    print('Value:', study.best_value)\n",
                "    print('Params:', study.best_params)\n",
                "    \n",
                "    return study\n",
                "\n",
                "# study = run_optimization()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "def test_model(model, test_loader, flatten=False):\n",
                "    model.eval()  # Set the model to evaluation mode\n",
                "    correct = 0\n",
                "    total = 0\n",
                "\n",
                "    # Disable gradient calculation for inference\n",
                "    with torch.no_grad():\n",
                "        for data, labels in test_loader:  # Use your test DataLoader here\n",
                "            if flatten:\n",
                "                data = data.view(data.size(0), -1)\n",
                "\n",
                "            # Forward pass: compute predictions\n",
                "            outputs = model(data)\n",
                "            \n",
                "            # Get the predicted class by finding the index of the max log-probability\n",
                "            _, predicted = torch.max(outputs.data, 1)\n",
                "\n",
                "            # Update total number of predictions\n",
                "            total += labels.size(0)\n",
                "            \n",
                "            # Update correct predictions\n",
                "            correct += (predicted == labels).sum().item()\n",
                "\n",
                "    # Calculate and print accuracy\n",
                "    accuracy = 100 * correct / total\n",
                "    print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [{
                "ename": "RuntimeError",
                "evalue": "mat1 and mat2 shapes cannot be multiplied (57600x300 and 270000x128)",
                "output_type": "error",
                "traceback": [
                    "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                    "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                    "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
                    "Cell \u001b[0;32mIn[24], line 11\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     10\u001b[0m    data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 11\u001b[0m    outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m    data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mview(data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m    \u001b[38;5;66;03m# Forward pass: compute predictions\u001b[39;00m\n",
                    "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                    "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                    "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
                    "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                    "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                    "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
                    "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (57600x300 and 270000x128)"
                ]
            }],
            "source": [
                "# Define the device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "# Instantiate the model and move it to the device\n",
                "model = CNN().to(device)  # Move model to GPU if available\n",
                "\n",
                "# Set up the optimizer and loss function\n",
                "optimizer = optim.AdamW(model.parameters(), lr=0.00020711569050274695)\n",
                "loss_fn = nn.CrossEntropyLoss() \n",
                "# Training and evaluation\n",
                "num_epochs = 8\n",
                "\n",
                "results = []\n",
                "for i in range(10):\n",
                "    result = k_fold(train_loader.dataset, 5, model, optimizer, loss_fn)\n",
                "    results.append(result)\n",
                "    \n",
                "print(results)\n",
                "# train_and_evaluate(train_loader, test_loader, model, optimizer, loss_fn, device, num_epochs)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1492 images from cherry class.\n",
      "Loaded 1494 images from strawberry class.\n",
      "Loaded 1494 images from tomato class.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define the classes\n",
    "classes = ['cherry', 'strawberry', 'tomato']\n",
    "data_dir = './train_data'\n",
    "\n",
    "# Dictionary to store the loaded images\n",
    "data = {}\n",
    "\n",
    "# List of images to exclude\n",
    "excluded_images = {\n",
    "    'cherry_0055.jpg',\n",
    "    'cherry_0105.jpg',\n",
    "    'cherry_0147.jpg',\n",
    "    'strawberry_0931.jpg',\n",
    "    'tomato_0087.jpg'\n",
    "}\n",
    "\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    images = []\n",
    "    \n",
    "    # Loop through all files in the class directory\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        if file_name.endswith('.jpg'):  # Check for image files\n",
    "            # Check if the file should be excluded\n",
    "            if file_name in excluded_images:\n",
    "                continue  # Skip this file\n",
    "            file_path = os.path.join(class_dir, file_name)\n",
    "            \n",
    "            # Open the image and append it to the list\n",
    "            img = Image.open(file_path)\n",
    "            images.append(img)\n",
    "    \n",
    "    # Store images for this class\n",
    "    data[class_name] = images\n",
    "\n",
    "\n",
    "# Example: Accessing images from the 'cherry' class\n",
    "print(f'Loaded {len(data[\"cherry\"])} images from cherry class.')\n",
    "print(f'Loaded {len(data[\"strawberry\"])} images from strawberry class.')\n",
    "print(f'Loaded {len(data[\"tomato\"])} images from tomato class.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1475 images from cherry class.\n",
      "Filtered 1477 images from strawberry class.\n",
      "Filtered 1476 images from tomato class.\n",
      "Removed 52 images in total.\n",
      "Filtered 4428 images in total.\n"
     ]
    }
   ],
   "source": [
    "# Define the target resolution\n",
    "target_size = (300, 300)\n",
    "\n",
    "# Dictionary to hold filtered data\n",
    "filtered_data = {}\n",
    "\n",
    "count = 0\n",
    "# Iterate through the classes\n",
    "for class_name, images in data.items():\n",
    "    filtered_images = []\n",
    "    \n",
    "    # Check each image for its resolution\n",
    "    for img in images:\n",
    "        if img.size == target_size:\n",
    "            filtered_images.append(img)  # Keep images that match 300x300\n",
    "        else:\n",
    "            count += 1\n",
    "    \n",
    "    # Store only the filtered images in the new dictionary\n",
    "    filtered_data[class_name] = filtered_images\n",
    "\n",
    "# Example: Accessing filtered images\n",
    "print(f'Filtered {len(filtered_data[\"cherry\"])} images from cherry class.')\n",
    "print(f'Filtered {len(filtered_data[\"strawberry\"])} images from strawberry class.')\n",
    "print(f'Filtered {len(filtered_data[\"tomato\"])} images from tomato class.')\n",
    "print(f'Removed {count} images in total.')\n",
    "print(f'Filtered {len(filtered_data[\"cherry\"])+len(filtered_data[\"strawberry\"])+len(filtered_data[\"tomato\"])} images in total.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images: 4428\n",
      "Processed images: 4427\n",
      "Removed Grayscale images: 1\n",
      "RGB images: 4426\n",
      "Outliers: 144\n",
      "Images in filtered_data: 4283\n",
      "\n",
      "Found 144 potential RGB channel-based outliers out of 4427 total images.\n",
      "Filtered data contains 4283 images after RGB channel-based filtering.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def detect_and_filter_rgb_outliers(image_data, thresholds):\n",
    "    filtered_data = defaultdict(list)\n",
    "    outliers = []\n",
    "    grayscale_count = 0\n",
    "    total_input_images = sum(len(images) for images in image_data.values())\n",
    "    \n",
    "    for class_name, images in image_data.items():\n",
    "        for img in images:\n",
    "            img_np = np.array(img)  # Convert image to NumPy array\n",
    "            \n",
    "            if len(img_np.shape) == 2:  # Grayscale image (only height and width)\n",
    "                grayscale_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Calculate the mean pixel intensity for each RGB channel\n",
    "            mean_channels = np.mean(img_np, axis=(0, 1))\n",
    "            \n",
    "            # Detect if any of the channels are outside their specific thresholds\n",
    "            condition = (mean_channels < [t[0] for t in thresholds]) | (mean_channels > [t[1] for t in thresholds])\n",
    "            if np.any(condition):\n",
    "                outliers.append(img)\n",
    "            else:\n",
    "                filtered_data[class_name].append(img)\n",
    "    \n",
    "    total_processed_images = sum(len(images) for images in filtered_data.values()) + len(outliers)\n",
    "    \n",
    "    print(f\"Input images: {total_input_images}\")\n",
    "    print(f\"Processed images: {total_processed_images}\")\n",
    "    print(f\"Removed Grayscale images: {grayscale_count}\")\n",
    "    print(f\"RGB images: {total_processed_images - grayscale_count}\")\n",
    "    print(f\"Outliers: {len(outliers)}\")\n",
    "    print(f\"Images in filtered_data: {sum(len(images) for images in filtered_data.values())}\")\n",
    "    \n",
    "    return dict(filtered_data), outliers\n",
    "\n",
    "# Define channel-specific thresholds based on the distributions\n",
    "thresholds = [\n",
    "    (27, 238),  # Red channel (low, high)\n",
    "    (14, 220),  # Green channel (low, high)\n",
    "    (8, 218)    # Blue channel (low, high)\n",
    "]\n",
    "\n",
    "# Use the optimized function with new thresholds\n",
    "filtered_data, rgb_outliers = detect_and_filter_rgb_outliers(filtered_data, thresholds)\n",
    "print(f'\\nFound {len(rgb_outliers)} potential RGB channel-based outliers out of {sum(len(images) for images in filtered_data.values()) + len(rgb_outliers)} total images.')\n",
    "print(f'Filtered data contains {sum(len(images) for images in filtered_data.values())} images after RGB channel-based filtering.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X shape: torch.Size([2998, 3, 300, 300]), Training y shape: torch.Size([2998])\n",
      "Testing X shape: torch.Size([1285, 3, 300, 300]), Testing y shape: torch.Size([1285])\n"
     ]
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts image to tensor and normalizes to [0, 1]\n",
    "    ])\n",
    "    X_data = []\n",
    "    y_labels = []\n",
    "\n",
    "    label_mapping = {\n",
    "        'cherry': 0,\n",
    "        'strawberry': 1,\n",
    "        'tomato': 2\n",
    "    }\n",
    "\n",
    "    # Step 1: Transform images directly without intermediate NumPy conversion\n",
    "    for label, images in data.items():\n",
    "        for img in images:\n",
    "            img_transformed = transform(img)  # Apply transformation to normalize and convert to tensor\n",
    "            X_data.append(img_transformed)\n",
    "            y_labels.append(label_mapping[label])\n",
    "\n",
    "    # Step 2: Stack tensors together\n",
    "    X = torch.stack(X_data)  # Now, X will be of shape [num_images, 3, 300, 300]\n",
    "    y = torch.tensor(y_labels)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_dataloaders():\n",
    "    X_train, X_test, y_train, y_test = normalize(filtered_data)\n",
    "    # Step 4: Create TensorDatasets and DataLoaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=8)\n",
    "    # Check shapes\n",
    "    print(f\"Training X shape: {X_train.shape}, Training y shape: {y_train.shape}\")\n",
    "    print(f\"Testing X shape: {X_test.shape}, Testing y shape: {y_test.shape}\")\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # First convolutional layer: input channels=3, output channels=16, kernel size=3x3\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Max pooling layer to downsample\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 75 * 75, 128)  # Adjusting for 300x300 input size after pooling\n",
    "        self.fc2 = nn.Linear(128, 3)  # Output size matches the number of classes (cherry, strawberry, tomato)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first conv layer, activation, and pooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Apply second conv layer, activation, and pooling\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        \n",
    "        # Flatten the output from convolutional layers\n",
    "        x = x.view(-1, 32 * 75 * 75)\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1, Loss: 1.9968378386999432\n",
      "Epoch 2, Loss: 1.0966160705215053\n",
      "Epoch 3, Loss: 1.0829339215629978\n",
      "Epoch 4, Loss: 1.0142132200692828\n",
      "Epoch 5, Loss: 0.8826032924024683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     fold_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_fold_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_fold_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     fold_results\u001b[38;5;241m.\u001b[39mappend(fold_accuracy)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Average accuracy across folds\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_loader, val_loader, model, optimizer, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_loader and test_loader are already defined with your data\n",
    "k_folds = 5\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_and_evaluate(train_loader, val_loader, model, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {val_acc}%')\n",
    "    return val_acc\n",
    "\n",
    "# Cross-validation\n",
    "fold_results = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create dataset from the dataloader\n",
    "dataset = train_loader.dataset\n",
    "num_samples = len(dataset)\n",
    "indices = list(range(num_samples))\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    \n",
    "    # Create data samplers and loaders for this fold\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_fold_loader = DataLoader(dataset, batch_size=train_loader.batch_size, sampler=train_sampler)\n",
    "    val_fold_loader = DataLoader(dataset, batch_size=train_loader.batch_size, sampler=val_sampler)\n",
    "    \n",
    "    # Reinitialize the model and optimizer for each fold\n",
    "    model = CNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    fold_accuracy = train_and_evaluate(train_fold_loader, val_fold_loader, model, optimizer, device)\n",
    "    fold_results.append(fold_accuracy)\n",
    "\n",
    "# Average accuracy across folds\n",
    "avg_accuracy = np.mean(fold_results)\n",
    "print(f'Cross-validation result: {avg_accuracy:.2f}% average accuracy across {k_folds} folds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion = nn.CrossEntropyLoss(), lr=0.001, num_epochs=10, flatten=False):\n",
    "    # Define loss function and optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            # Flatten the input images\n",
    "            if flatten:\n",
    "                data = data.view(data.size(0), -1)  # [batch_size, 270000]\n",
    "\n",
    "            # Forward pass: compute predictions\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass: compute gradients\n",
    "            optimizer.zero_grad()  # Clear the previous gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print average loss for the epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "def get_trained_mlp(train_loader):\n",
    "    # Define input, hidden, and output sizes\n",
    "    input_size = 3 * 300 * 300  # 270,000 for RGB images\n",
    "    hidden_size = 128  # You can tune this based on your needs\n",
    "    output_size = 3  # 3 classes: 'cherry', 'strawberry', 'tomato'\n",
    "    # Instantiate the model\n",
    "    model = MLP(input_size, hidden_size, output_size)\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "    return train(model, train_loader, criterion, lr=0.001, num_epochs=10, flatten=True)\n",
    "\n",
    "def get_trained_cnn(train_loader):\n",
    "    model = CNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return train(model, train_loader, criterion, lr=0.001, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, flatten=False):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Disable gradient calculation for inference\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:  # Use your test DataLoader here\n",
    "            if flatten:\n",
    "                data = data.view(data.size(0), -1)\n",
    "            # Forward pass: compute predictions\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # Get the predicted class by finding the index of the max log-probability\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update total number of predictions\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Update correct predictions\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4775\n",
      "Epoch [2/10], Loss: 1.0897\n",
      "Epoch [3/10], Loss: 1.0053\n",
      "Epoch [4/10], Loss: 0.8376\n",
      "Epoch [5/10], Loss: 0.6145\n",
      "Epoch [6/10], Loss: 0.3642\n",
      "Epoch [7/10], Loss: 0.1946\n",
      "Epoch [8/10], Loss: 0.0777\n",
      "Epoch [9/10], Loss: 0.0370\n",
      "Epoch [10/10], Loss: 0.0164\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "cnn_model = get_trained_cnn(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 54.55%\n"
     ]
    }
   ],
   "source": [
    "test_model(cnn_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

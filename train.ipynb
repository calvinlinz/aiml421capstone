{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 1492 images from cherry class.\n",
                        "Loaded 1494 images from strawberry class.\n",
                        "Loaded 1494 images from tomato class.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from PIL import Image\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from torchvision import transforms\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "\n",
                "# Define the classes\n",
                "classes = ['cherry', 'strawberry', 'tomato']\n",
                "data_dir = './train_data'\n",
                "\n",
                "# Dictionary to store the loaded images\n",
                "data = {}\n",
                "\n",
                "# List of images to exclude\n",
                "excluded_images = {\n",
                "    'cherry_0055.jpg',\n",
                "    'cherry_0105.jpg',\n",
                "    'cherry_0147.jpg',\n",
                "    'strawberry_0931.jpg',\n",
                "    'tomato_0087.jpg'\n",
                "}\n",
                "\n",
                "for class_name in classes:\n",
                "    class_dir = os.path.join(data_dir, class_name)\n",
                "    images = []\n",
                "    \n",
                "    # Loop through all files in the class directory\n",
                "    for file_name in os.listdir(class_dir):\n",
                "        if file_name.endswith('.jpg'):  # Check for image files\n",
                "            # Check if the file should be excluded\n",
                "            if file_name in excluded_images:\n",
                "                continue  # Skip this file\n",
                "            file_path = os.path.join(class_dir, file_name)\n",
                "            \n",
                "            # Open the image and append it to the list\n",
                "            img = Image.open(file_path)\n",
                "            images.append(img)\n",
                "    \n",
                "    # Store images for this class\n",
                "    data[class_name] = images\n",
                "\n",
                "\n",
                "# Example: Accessing images from the 'cherry' class\n",
                "print(f'Loaded {len(data[\"cherry\"])} images from cherry class.')\n",
                "print(f'Loaded {len(data[\"strawberry\"])} images from strawberry class.')\n",
                "print(f'Loaded {len(data[\"tomato\"])} images from tomato class.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import random\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "def set_seed(seed):\n",
                "    random.seed(seed)  # For Python random\n",
                "    np.random.seed(seed)  # For NumPy random\n",
                "    torch.manual_seed(seed)  # For PyTorch CPU random\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)  # For PyTorch GPU random\n",
                "        torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n",
                "    torch.backends.cudnn.deterministic = True  # Make sure CUDA computations are deterministic\n",
                "    torch.backends.cudnn.benchmark = False  # Disable benchmark mode to make it reproducible\n",
                "\n",
                "# Set a seed value, e.g., 42\n",
                "set_seed(42)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Filtered 1475 images from cherry class.\n",
                        "Filtered 1477 images from strawberry class.\n",
                        "Filtered 1476 images from tomato class.\n",
                        "Removed 52 images in total.\n",
                        "Filtered 4428 images in total.\n"
                    ]
                }
            ],
            "source": [
                "# Define the target resolution\n",
                "target_size = (300, 300)\n",
                "\n",
                "# Dictionary to hold filtered data\n",
                "filtered_data = {}\n",
                "\n",
                "count = 0\n",
                "# Iterate through the classes\n",
                "for class_name, images in data.items():\n",
                "    filtered_images = []\n",
                "    \n",
                "    # Check each image for its resolution\n",
                "    for img in images:\n",
                "        if img.size == target_size:\n",
                "            filtered_images.append(img)  # Keep images that match 300x300\n",
                "        else:\n",
                "            count += 1\n",
                "    \n",
                "    # Store only the filtered images in the new dictionary\n",
                "    filtered_data[class_name] = filtered_images\n",
                "\n",
                "# Example: Accessing filtered images\n",
                "print(f'Filtered {len(filtered_data[\"cherry\"])} images from cherry class.')\n",
                "print(f'Filtered {len(filtered_data[\"strawberry\"])} images from strawberry class.')\n",
                "print(f'Filtered {len(filtered_data[\"tomato\"])} images from tomato class.')\n",
                "print(f'Removed {count} images in total.')\n",
                "print(f'Filtered {len(filtered_data[\"cherry\"])+len(filtered_data[\"strawberry\"])+len(filtered_data[\"tomato\"])} images in total.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input images: 4428\n",
                        "Processed images: 4427\n",
                        "Removed Grayscale images: 1\n",
                        "RGB images: 4426\n",
                        "Outliers: 144\n",
                        "Images in filtered_data: 4283\n",
                        "\n",
                        "Found 144 potential RGB channel-based outliers out of 4427 total images.\n",
                        "Filtered data contains 4283 images after RGB channel-based filtering.\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "from collections import defaultdict\n",
                "\n",
                "def detect_and_filter_rgb_outliers(image_data, thresholds):\n",
                "    filtered_data = defaultdict(list)\n",
                "    outliers = []\n",
                "    grayscale_count = 0\n",
                "    total_input_images = sum(len(images) for images in image_data.values())\n",
                "    \n",
                "    for class_name, images in image_data.items():\n",
                "        for img in images:\n",
                "            img_np = np.array(img)  # Convert image to NumPy array\n",
                "            \n",
                "            if len(img_np.shape) == 2:  # Grayscale image (only height and width)\n",
                "                grayscale_count += 1\n",
                "                continue\n",
                "            \n",
                "            # Calculate the mean pixel intensity for each RGB channel\n",
                "            mean_channels = np.mean(img_np, axis=(0, 1))\n",
                "            \n",
                "            # Detect if any of the channels are outside their specific thresholds\n",
                "            condition = (mean_channels < [t[0] for t in thresholds]) | (mean_channels > [t[1] for t in thresholds])\n",
                "            if np.any(condition):\n",
                "                outliers.append(img)\n",
                "            else:\n",
                "                filtered_data[class_name].append(img)\n",
                "    \n",
                "    total_processed_images = sum(len(images) for images in filtered_data.values()) + len(outliers)\n",
                "    \n",
                "    print(f\"Input images: {total_input_images}\")\n",
                "    print(f\"Processed images: {total_processed_images}\")\n",
                "    print(f\"Removed Grayscale images: {grayscale_count}\")\n",
                "    print(f\"RGB images: {total_processed_images - grayscale_count}\")\n",
                "    print(f\"Outliers: {len(outliers)}\")\n",
                "    print(f\"Images in filtered_data: {sum(len(images) for images in filtered_data.values())}\")\n",
                "    \n",
                "    return dict(filtered_data), outliers\n",
                "\n",
                "# Define channel-specific thresholds based on the distributions\n",
                "thresholds = [\n",
                "    (27, 238),  # Red channel (low, high)\n",
                "    (14, 220),  # Green channel (low, high)\n",
                "    (8, 218)    # Blue channel (low, high)\n",
                "]\n",
                "\n",
                "# Use the optimized function with new thresholds\n",
                "filtered_data, rgb_outliers = detect_and_filter_rgb_outliers(filtered_data, thresholds)\n",
                "print(f'\\nFound {len(rgb_outliers)} potential RGB channel-based outliers out of {sum(len(images) for images in filtered_data.values()) + len(rgb_outliers)} total images.')\n",
                "print(f'Filtered data contains {sum(len(images) for images in filtered_data.values())} images after RGB channel-based filtering.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def normalize(data):\n",
                "    transform = transforms.Compose([\n",
                "        transforms.ToTensor(),  # Converts image to tensor and normalizes to [0, 1]\n",
                "    ])\n",
                "    X_data = []\n",
                "    y_labels = []\n",
                "\n",
                "    label_mapping = {\n",
                "        'cherry': 0,\n",
                "        'strawberry': 1,\n",
                "        'tomato': 2\n",
                "    }\n",
                "\n",
                "    # Step 1: Transform images directly without intermediate NumPy conversion\n",
                "    for label, images in data.items():\n",
                "        for img in images:\n",
                "            img_transformed = transform(img)  # Apply transformation to normalize and convert to tensor\n",
                "            X_data.append(img_transformed)\n",
                "            y_labels.append(label_mapping[label])\n",
                "\n",
                "    # Step 2: Stack tensors together\n",
                "    X = torch.stack(X_data)  # Now, X will be of shape [num_images, 3, 300, 300]\n",
                "    y = torch.tensor(y_labels)\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "    return X_train, X_test, y_train, y_test\n",
                "\n",
                "def get_dataloaders(batch_size):\n",
                "    X_train, X_test, y_train, y_test = normalize(filtered_data)\n",
                "    # Step 4: Create TensorDatasets and DataLoaders\n",
                "    train_dataset = TensorDataset(X_train, y_train)\n",
                "    test_dataset = TensorDataset(X_test, y_test)\n",
                "\n",
                "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
                "    # Check shapes\n",
                "    print(f\"Training X shape: {X_train.shape}, Training y shape: {y_train.shape}\")\n",
                "    print(f\"Testing X shape: {X_test.shape}, Testing y shape: {y_test.shape}\")\n",
                "    return train_loader, test_loader\n",
                "\n",
                "\n",
                "batch_size = 64\n",
                "train_loader, test_loader = get_dataloaders(batch_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "class MLP(nn.Module):\n",
                "    def __init__(self, input_size=300*300*3, hidden_size=512, output_size=3):\n",
                "        super(MLP, self).__init__()\n",
                "        self.flatten = nn.Flatten()\n",
                "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
                "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
                "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
                "        self.relu = nn.ReLU()\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.flatten(x)  # Flatten the input image\n",
                "        x = self.relu(self.fc1(x))\n",
                "        x = self.relu(self.fc2(x))\n",
                "        x = self.fc3(x)\n",
                "        return x\n",
                "\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self):  # Use dropout_rate instead of decay for clarity\n",
                "        super(CNN, self,).__init__()\n",
                "\n",
                "        # First convolutional layer: input channels=3, output channels=16, kernel size=3x3\n",
                "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
                "\n",
                "        # Max pooling layer to downsample\n",
                "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "\n",
                "        # Fully connected layers\n",
                "        self.fc1 = nn.Linear(32 * 75 * 75, 128)  # Adjusting for 300x300 input size after pooling\n",
                "        self.fc2 = nn.Linear(128, 3)  # Output size matches the number of classes (cherry, strawberry, tomato)\n",
                "\n",
                "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        # self.dropout = nn.Dropout(p=0.0000147)\n",
                "\n",
                "        # Activation function\n",
                "        self.relu = nn.ReLU()\n",
                "\n",
                "    def forward(self, x):\n",
                "        # Apply first conv layer, activation, and pooling\n",
                "        x = self.pool(self.relu(self.conv1(x)))\n",
                "\n",
                "        # Apply second conv layer, activation, and pooling\n",
                "        x = self.pool(self.relu(self.conv2(x)))\n",
                "\n",
                "        # Flatten the output from convolutional layers\n",
                "        x = x.view(-1, 32 * 75 * 75)\n",
                "\n",
                "        # Apply first fully connected layer with dropout\n",
                "        x = self.relu(self.fc1(x))\n",
                "        # x = self.dropout(x)  # Apply dropout here\n",
                "\n",
                "        # Apply second fully connected layer\n",
                "        x = self.fc2(x)\n",
                "\n",
                "        return x\n",
                "\n",
                "class FocalLoss(nn.Module):\n",
                "    def __init__(self, alpha=1, gamma=2):\n",
                "        super(FocalLoss, self).__init__()\n",
                "        self.alpha = alpha\n",
                "        self.gamma = gamma\n",
                "        self.ce_loss = nn.CrossEntropyLoss()\n",
                "\n",
                "    def forward(self, inputs, targets):\n",
                "        ce_loss = self.ce_loss(inputs, targets)\n",
                "        pt = torch.exp(-ce_loss)\n",
                "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
                "        return focal_loss\n",
                "    \n",
                "\n",
                "class LabelSmoothingLoss(nn.Module):\n",
                "    def __init__(self, smoothing=0.1, num_classes=3):\n",
                "        super().__init__()\n",
                "        self.smoothing = smoothing\n",
                "        self.num_classes = num_classes\n",
                "        self.confidence = 1.0 - smoothing\n",
                "        \n",
                "    def forward(self, pred, target):\n",
                "        pred = F.log_softmax(pred, dim=-1)\n",
                "        \n",
                "        # Create smoothed labels\n",
                "        with torch.no_grad():\n",
                "            true_dist = torch.zeros_like(pred)\n",
                "            true_dist.fill_(self.smoothing / (self.num_classes - 1))\n",
                "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
                "        \n",
                "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
                "from sklearn.model_selection import KFold\n",
                "import numpy as np\n",
                "import optuna\n",
                "import optuna.visualization as vis\n",
                "import matplotlib.pyplot as plt\n",
                "import torchvision.models as models\n",
                "\n",
                "\n",
                "def evaluate(model, val_loader, device ):\n",
                "    model.eval()\n",
                "    total, correct = 0, 0\n",
                "    val_accuracies = []\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for inputs, labels in val_loader:\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "            outputs = model(inputs)\n",
                "            \n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "\n",
                "    val_acc = 100 * correct / total\n",
                "    \n",
                "    # Store validation metrics\n",
                "    val_accuracies.append(val_acc)\n",
                "    return val_acc\n",
                "    \n",
                "def train_and_evaluate(train_loader, val_loader, model, optimizer, loss_function, device, num_epochs):\n",
                "    train_losses = []\n",
                "    val_losses = []\n",
                "    train_accuracies = []\n",
                "    val_accuracies = []\n",
                "    \n",
                "    for epoch in range(num_epochs):\n",
                "        model.train()\n",
                "        total, correct = 0, 0\n",
                "        running_loss = 0.0\n",
                "\n",
                "        for inputs, labels in train_loader:\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(inputs)\n",
                "            loss = loss_function(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "            running_loss += loss.item()\n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "\n",
                "        epoch_train_loss = running_loss / len(train_loader)\n",
                "        train_acc = 100 * correct / total\n",
                "        \n",
                "        # Store training metrics\n",
                "        train_losses.append(epoch_train_loss)\n",
                "        train_accuracies.append(train_acc)\n",
                "\n",
                "        # Validation phase\n",
                "        model.eval()\n",
                "        total, correct = 0, 0\n",
                "        running_loss = 0.0\n",
                "\n",
                "        with torch.no_grad():\n",
                "            for inputs, labels in val_loader:\n",
                "                inputs, labels = inputs.to(device), labels.to(device)\n",
                "                outputs = model(inputs)\n",
                "                loss = loss_function(outputs, labels)\n",
                "                running_loss += loss.item()\n",
                "                \n",
                "                _, predicted = torch.max(outputs, 1)\n",
                "                total += labels.size(0)\n",
                "                correct += (predicted == labels).sum().item()\n",
                "\n",
                "        epoch_val_loss = running_loss / len(val_loader)\n",
                "        val_acc = 100 * correct / total\n",
                "        \n",
                "        # Store validation metrics\n",
                "        val_losses.append(epoch_val_loss)\n",
                "        val_accuracies.append(val_acc)\n",
                "\n",
                "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
                "        print(f\"Training - Loss: {epoch_train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
                "        print(f\"Validation - Loss: {epoch_val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
                "        print(\"-\" * 50)\n",
                "    torch.save(model, 'model_final.pth')\n",
                "    return val_acc\n",
                "\n",
                "\n",
                "def k_fold(dataset, k_folds, trial=None):\n",
                "    indices = list(range(len(dataset)))\n",
                "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
                "    fold_accuracies = []\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "    for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
                "        train_sampler = SubsetRandomSampler(train_idx)\n",
                "        val_sampler = SubsetRandomSampler(val_idx)\n",
                "        model = CNN()\n",
                "        # model.fc = nn.Linear(model.fc.in_features, 3)\n",
                "        model = model.to(device)    \n",
                "        loss_function = LabelSmoothingLoss()\n",
                "        optimizer = optim.AdamW(model.parameters(), lr=0.00020711569050274695)\n",
                "\n",
                "        train_fold_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
                "        val_fold_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
                "\n",
                "        val_accuracy = train_and_evaluate(\n",
                "            train_fold_loader, val_fold_loader,\n",
                "            model, optimizer, loss_function,\n",
                "            device, num_epochs=8\n",
                "        )\n",
                "\n",
                "        fold_accuracies.append(val_accuracy)\n",
                "\n",
                "        if trial:\n",
                "            trial.report(val_accuracy, fold)\n",
                "            if trial.should_prune():\n",
                "                raise optuna.TrialPruned()\n",
                "    return fold_accuracies\n",
                "\n",
                "\n",
                "def objective(trial):\n",
                "    k_folds = 5\n",
                "    dataset = train_loader.dataset\n",
                "    fold_accuracies = k_fold(dataset, k_folds,trial)\n",
                "    return np.mean(fold_accuracies)\n",
                "\n",
                "def run_optimization(n_trials=10):\n",
                "    study = optuna.create_study(\n",
                "        direction='maximize',\n",
                "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5)\n",
                "    )\n",
                "    study.optimize(\n",
                "        objective,\n",
                "        n_trials=n_trials,\n",
                "        timeout=3600  # 1 hour timeout\n",
                "    )\n",
                "    \n",
                "    print('\\nBest trial:')\n",
                "    print('Value:', study.best_value)\n",
                "    print('Params:', study.best_params)\n",
                "    \n",
                "    return study\n",
                "\n",
                "\n",
                "# study = run_optimization()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "ename": "OutOfMemoryError",
                    "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 15.25 MiB is free. Process 3534442 has 2.76 GiB memory in use. Process 3665996 has 9.38 GiB memory in use. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 11.06 GiB is allocated by PyTorch, and 205.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00020711569050274695\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# result = k_fold(train_loader.dataset, 5)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m result1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# result = train_and_evaluate(train_loader, test_loader, model, optimizer, loss_function, device, num_epochs)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# result = k_fold(train_loader.dataset, 5)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result1)\n",
                        "Cell \u001b[0;32mIn[9], line 48\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_loader, val_loader, model, optimizer, loss_function, device, num_epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[1;32m     50\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torchvision/models/resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torchvision/models/resnet.py:155\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[0;32m--> 155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 15.25 MiB is free. Process 3534442 has 2.76 GiB memory in use. Process 3665996 has 9.38 GiB memory in use. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 11.06 GiB is allocated by PyTorch, and 205.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
                    ]
                }
            ],
            "source": [
                "num_epochs = 8\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).to(device)\n",
                "model.fc = nn.Linear(model.fc.in_features, 3).to(device)\n",
                "loss_function = LabelSmoothingLoss()\n",
                "optimizer = optim.AdamW(model.parameters(), lr=0.00020711569050274695)\n",
                "\n",
                "# result = k_fold(train_loader.dataset, 5)\n",
                "result1 = train_and_evaluate(train_loader,test_loader,model,optimizer,loss_function,device,num_epochs)\n",
                "    \n",
                "# result = train_and_evaluate(train_loader, test_loader, model, optimizer, loss_function, device, num_epochs)\n",
                "# result = k_fold(train_loader.dataset, 5)\n",
                "print(\"acc: \", result1)\n",
                "# print(result)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}

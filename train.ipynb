{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 1492 images from cherry class.\n",
                        "Loaded 1494 images from strawberry class.\n",
                        "Loaded 1494 images from tomato class.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from PIL import Image\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "from torchvision import transforms\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "\n",
                "# Define the classes\n",
                "classes = ['cherry', 'strawberry', 'tomato']\n",
                "data_dir = './train_data'\n",
                "\n",
                "# Dictionary to store the loaded images\n",
                "data = {}\n",
                "\n",
                "# List of images to exclude\n",
                "excluded_images = {\n",
                "    'cherry_0055.jpg',\n",
                "    'cherry_0105.jpg',\n",
                "    'cherry_0147.jpg',\n",
                "    'strawberry_0931.jpg',\n",
                "    'tomato_0087.jpg'\n",
                "}\n",
                "\n",
                "for class_name in classes:\n",
                "    class_dir = os.path.join(data_dir, class_name)\n",
                "    images = []\n",
                "    \n",
                "    # Loop through all files in the class directory\n",
                "    for file_name in os.listdir(class_dir):\n",
                "        if file_name.endswith('.jpg'):  # Check for image files\n",
                "            # Check if the file should be excluded\n",
                "            if file_name in excluded_images:\n",
                "                continue  # Skip this file\n",
                "            file_path = os.path.join(class_dir, file_name)\n",
                "            \n",
                "            # Open the image and append it to the list\n",
                "            img = Image.open(file_path)\n",
                "            images.append(img)\n",
                "    \n",
                "    # Store images for this class\n",
                "    data[class_name] = images\n",
                "\n",
                "\n",
                "# Example: Accessing images from the 'cherry' class\n",
                "print(f'Loaded {len(data[\"cherry\"])} images from cherry class.')\n",
                "print(f'Loaded {len(data[\"strawberry\"])} images from strawberry class.')\n",
                "print(f'Loaded {len(data[\"tomato\"])} images from tomato class.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import random\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "def set_seed(seed):\n",
                "    random.seed(seed)  # For Python random\n",
                "    np.random.seed(seed)  # For NumPy random\n",
                "    torch.manual_seed(seed)  # For PyTorch CPU random\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)  # For PyTorch GPU random\n",
                "        torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n",
                "    torch.backends.cudnn.deterministic = True  # Make sure CUDA computations are deterministic\n",
                "    torch.backends.cudnn.benchmark = False  # Disable benchmark mode to make it reproducible\n",
                "\n",
                "# Set a seed value, e.g., 42\n",
                "set_seed(42)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Filtered 1475 images from cherry class.\n",
                        "Filtered 1477 images from strawberry class.\n",
                        "Filtered 1476 images from tomato class.\n",
                        "Removed 52 images in total.\n",
                        "Filtered 4428 images in total.\n"
                    ]
                }
            ],
            "source": [
                "# Define the target resolution\n",
                "target_size = (300, 300)\n",
                "\n",
                "# Dictionary to hold filtered data\n",
                "filtered_data = {}\n",
                "\n",
                "count = 0\n",
                "# Iterate through the classes\n",
                "for class_name, images in data.items():\n",
                "    filtered_images = []\n",
                "    \n",
                "    # Check each image for its resolution\n",
                "    for img in images:\n",
                "        if img.size == target_size:\n",
                "            filtered_images.append(img)  # Keep images that match 300x300\n",
                "        else:\n",
                "            count += 1\n",
                "    \n",
                "    # Store only the filtered images in the new dictionary\n",
                "    filtered_data[class_name] = filtered_images\n",
                "\n",
                "# Example: Accessing filtered images\n",
                "print(f'Filtered {len(filtered_data[\"cherry\"])} images from cherry class.')\n",
                "print(f'Filtered {len(filtered_data[\"strawberry\"])} images from strawberry class.')\n",
                "print(f'Filtered {len(filtered_data[\"tomato\"])} images from tomato class.')\n",
                "print(f'Removed {count} images in total.')\n",
                "print(f'Filtered {len(filtered_data[\"cherry\"])+len(filtered_data[\"strawberry\"])+len(filtered_data[\"tomato\"])} images in total.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input images: 4428\n",
                        "Processed images: 4427\n",
                        "Removed Grayscale images: 1\n",
                        "RGB images: 4426\n",
                        "Outliers: 144\n",
                        "Images in filtered_data: 4283\n",
                        "\n",
                        "Found 144 potential RGB channel-based outliers out of 4427 total images.\n",
                        "Filtered data contains 4283 images after RGB channel-based filtering.\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "from collections import defaultdict\n",
                "\n",
                "def detect_and_filter_rgb_outliers(image_data, thresholds):\n",
                "    filtered_data = defaultdict(list)\n",
                "    outliers = []\n",
                "    grayscale_count = 0\n",
                "    total_input_images = sum(len(images) for images in image_data.values())\n",
                "    \n",
                "    for class_name, images in image_data.items():\n",
                "        for img in images:\n",
                "            img_np = np.array(img)  # Convert image to NumPy array\n",
                "            \n",
                "            if len(img_np.shape) == 2:  # Grayscale image (only height and width)\n",
                "                grayscale_count += 1\n",
                "                continue\n",
                "            \n",
                "            # Calculate the mean pixel intensity for each RGB channel\n",
                "            mean_channels = np.mean(img_np, axis=(0, 1))\n",
                "            \n",
                "            # Detect if any of the channels are outside their specific thresholds\n",
                "            condition = (mean_channels < [t[0] for t in thresholds]) | (mean_channels > [t[1] for t in thresholds])\n",
                "            if np.any(condition):\n",
                "                outliers.append(img)\n",
                "            else:\n",
                "                filtered_data[class_name].append(img)\n",
                "    \n",
                "    total_processed_images = sum(len(images) for images in filtered_data.values()) + len(outliers)\n",
                "    \n",
                "    print(f\"Input images: {total_input_images}\")\n",
                "    print(f\"Processed images: {total_processed_images}\")\n",
                "    print(f\"Removed Grayscale images: {grayscale_count}\")\n",
                "    print(f\"RGB images: {total_processed_images - grayscale_count}\")\n",
                "    print(f\"Outliers: {len(outliers)}\")\n",
                "    print(f\"Images in filtered_data: {sum(len(images) for images in filtered_data.values())}\")\n",
                "    \n",
                "    return dict(filtered_data), outliers\n",
                "\n",
                "# Define channel-specific thresholds based on the distributions\n",
                "thresholds = [\n",
                "    (27, 238),  # Red channel (low, high)\n",
                "    (14, 220),  # Green channel (low, high)\n",
                "    (8, 218)    # Blue channel (low, high)\n",
                "]\n",
                "\n",
                "# Use the optimized function with new thresholds\n",
                "filtered_data, rgb_outliers = detect_and_filter_rgb_outliers(filtered_data, thresholds)\n",
                "print(f'\\nFound {len(rgb_outliers)} potential RGB channel-based outliers out of {sum(len(images) for images in filtered_data.values()) + len(rgb_outliers)} total images.')\n",
                "print(f'Filtered data contains {sum(len(images) for images in filtered_data.values())} images after RGB channel-based filtering.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training X shape: torch.Size([2998, 3, 300, 300]), Training y shape: torch.Size([2998])\n",
                        "Testing X shape: torch.Size([1285, 3, 300, 300]), Testing y shape: torch.Size([1285])\n"
                    ]
                }
            ],
            "source": [
                "def normalize(data):\n",
                "    transform = transforms.Compose([\n",
                "        transforms.ToTensor(),  # Converts image to tensor and normalizes to [0, 1]\n",
                "    ])\n",
                "    X_data = []\n",
                "    y_labels = []\n",
                "\n",
                "    label_mapping = {\n",
                "        'cherry': 0,\n",
                "        'strawberry': 1,\n",
                "        'tomato': 2\n",
                "    }\n",
                "\n",
                "    # Step 1: Transform images directly without intermediate NumPy conversion\n",
                "    for label, images in data.items():\n",
                "        for img in images:\n",
                "            img_transformed = transform(img)  # Apply transformation to normalize and convert to tensor\n",
                "            X_data.append(img_transformed)\n",
                "            y_labels.append(label_mapping[label])\n",
                "\n",
                "    # Step 2: Stack tensors together\n",
                "    X = torch.stack(X_data)  # Now, X will be of shape [num_images, 3, 300, 300]\n",
                "    y = torch.tensor(y_labels)\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
                "    return X_train, X_test, y_train, y_test\n",
                "\n",
                "def get_dataloaders(batch_size):\n",
                "    X_train, X_test, y_train, y_test = normalize(filtered_data)\n",
                "    # Step 4: Create TensorDatasets and DataLoaders\n",
                "    train_dataset = TensorDataset(X_train, y_train)\n",
                "    test_dataset = TensorDataset(X_test, y_test)\n",
                "\n",
                "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
                "    # Check shapes\n",
                "    print(f\"Training X shape: {X_train.shape}, Training y shape: {y_train.shape}\")\n",
                "    print(f\"Testing X shape: {X_test.shape}, Testing y shape: {y_test.shape}\")\n",
                "    return train_loader, test_loader\n",
                "\n",
                "\n",
                "batch_size = 64\n",
                "train_loader, test_loader = get_dataloaders(batch_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "class MLP(nn.Module):\n",
                "    def __init__(self, input_size=300*300*3, hidden_size=512, output_size=3):\n",
                "        super(MLP, self).__init__()\n",
                "        self.flatten = nn.Flatten()\n",
                "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
                "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
                "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
                "        self.relu = nn.ReLU()\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.flatten(x)  # Flatten the input image\n",
                "        x = self.relu(self.fc1(x))\n",
                "        x = self.relu(self.fc2(x))\n",
                "        x = self.fc3(x)\n",
                "        return x\n",
                "\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self):  # Use dropout_rate instead of decay for clarity\n",
                "        super(CNN, self,).__init__()\n",
                "\n",
                "        # First convolutional layer: input channels=3, output channels=16, kernel size=3x3\n",
                "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
                "\n",
                "        # Max pooling layer to downsample\n",
                "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "\n",
                "        # Fully connected layers\n",
                "        self.fc1 = nn.Linear(32 * 75 * 75, 128)  # Adjusting for 300x300 input size after pooling\n",
                "        self.fc2 = nn.Linear(128, 3)  # Output size matches the number of classes (cherry, strawberry, tomato)\n",
                "\n",
                "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
                "        self.dropout = nn.Dropout(p=0.0000147)\n",
                "\n",
                "        # Activation function\n",
                "        self.relu = nn.ReLU()\n",
                "\n",
                "    def forward(self, x):\n",
                "        # Apply first conv layer, activation, and pooling\n",
                "        x = self.pool(self.relu(self.conv1(x)))\n",
                "\n",
                "        # Apply second conv layer, activation, and pooling\n",
                "        x = self.pool(self.relu(self.conv2(x)))\n",
                "\n",
                "        # Flatten the output from convolutional layers\n",
                "        x = x.view(-1, 32 * 75 * 75)\n",
                "\n",
                "        # Apply first fully connected layer with dropout\n",
                "        x = self.relu(self.fc1(x))\n",
                "        x = self.dropout(x)  # Apply dropout here\n",
                "\n",
                "        # Apply second fully connected layer\n",
                "        x = self.fc2(x)\n",
                "\n",
                "        return x\n",
                "\n",
                "class FocalLoss(nn.Module):\n",
                "    def __init__(self, alpha=1, gamma=2):\n",
                "        super(FocalLoss, self).__init__()\n",
                "        self.alpha = alpha\n",
                "        self.gamma = gamma\n",
                "        self.ce_loss = nn.CrossEntropyLoss()\n",
                "\n",
                "    def forward(self, inputs, targets):\n",
                "        ce_loss = self.ce_loss(inputs, targets)\n",
                "        pt = torch.exp(-ce_loss)\n",
                "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
                "        return focal_loss\n",
                "    \n",
                "\n",
                "class LabelSmoothingLoss(nn.Module):\n",
                "    def __init__(self, smoothing=0.1, num_classes=3):\n",
                "        \"\"\"\n",
                "        Label smoothing loss for multi-class classification\n",
                "        \n",
                "        Args:\n",
                "            smoothing (float): smoothing factor, typically between 0.0 and 0.2\n",
                "            num_classes (int): number of classes in your dataset\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.smoothing = smoothing\n",
                "        self.num_classes = num_classes\n",
                "        self.confidence = 1.0 - smoothing\n",
                "        \n",
                "    def forward(self, pred, target):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            pred: predictions from model (N, C)\n",
                "            target: target labels (N,)\n",
                "        \"\"\"\n",
                "        pred = F.log_softmax(pred, dim=-1)\n",
                "        \n",
                "        # Create smoothed labels\n",
                "        with torch.no_grad():\n",
                "            true_dist = torch.zeros_like(pred)\n",
                "            true_dist.fill_(self.smoothing / (self.num_classes - 1))\n",
                "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
                "        \n",
                "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
                "from sklearn.model_selection import KFold\n",
                "import numpy as np\n",
                "import optuna\n",
                "import optuna.visualization as vis\n",
                "import matplotlib.pyplot as plt\n",
                "import torchvision.models as models\n",
                "\n",
                "\n",
                "def evaluate(model, val_loader, device ):\n",
                "    model.eval()\n",
                "    total, correct = 0, 0\n",
                "    val_accuracies = []\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for inputs, labels in val_loader:\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "            outputs = model(inputs)\n",
                "            \n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "\n",
                "    val_acc = 100 * correct / total\n",
                "    \n",
                "    # Store validation metrics\n",
                "    val_accuracies.append(val_acc)\n",
                "    return val_acc\n",
                "    \n",
                "def train_and_evaluate(train_loader, val_loader, model, optimizer, loss_function, device, num_epochs):\n",
                "    train_losses = []\n",
                "    val_losses = []\n",
                "    train_accuracies = []\n",
                "    val_accuracies = []\n",
                "    \n",
                "    for epoch in range(num_epochs):\n",
                "        model.train()\n",
                "        total, correct = 0, 0\n",
                "        running_loss = 0.0\n",
                "\n",
                "        for inputs, labels in train_loader:\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(inputs)\n",
                "            loss = loss_function(outputs, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "\n",
                "            running_loss += loss.item()\n",
                "            _, predicted = torch.max(outputs, 1)\n",
                "            total += labels.size(0)\n",
                "            correct += (predicted == labels).sum().item()\n",
                "\n",
                "        epoch_train_loss = running_loss / len(train_loader)\n",
                "        train_acc = 100 * correct / total\n",
                "        \n",
                "        # Store training metrics\n",
                "        train_losses.append(epoch_train_loss)\n",
                "        train_accuracies.append(train_acc)\n",
                "\n",
                "        # Validation phase\n",
                "        model.eval()\n",
                "        total, correct = 0, 0\n",
                "        running_loss = 0.0\n",
                "\n",
                "        with torch.no_grad():\n",
                "            for inputs, labels in val_loader:\n",
                "                inputs, labels = inputs.to(device), labels.to(device)\n",
                "                outputs = model(inputs)\n",
                "                loss = loss_function(outputs, labels)\n",
                "                running_loss += loss.item()\n",
                "                \n",
                "                _, predicted = torch.max(outputs, 1)\n",
                "                total += labels.size(0)\n",
                "                correct += (predicted == labels).sum().item()\n",
                "\n",
                "        epoch_val_loss = running_loss / len(val_loader)\n",
                "        val_acc = 100 * correct / total\n",
                "        \n",
                "        # Store validation metrics\n",
                "        val_losses.append(epoch_val_loss)\n",
                "        val_accuracies.append(val_acc)\n",
                "\n",
                "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
                "        print(f\"Training - Loss: {epoch_train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
                "        print(f\"Validation - Loss: {epoch_val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
                "        print(\"-\" * 50)\n",
                "    return val_acc\n",
                "\n",
                "\n",
                "def k_fold(dataset, k_folds, trial=None):\n",
                "    indices = list(range(len(dataset)))\n",
                "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
                "    fold_accuracies = []\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    highest_acc = 0\n",
                "    best_model = None\n",
                "    for fold, (train_idx, val_idx) in enumerate(kfold.split(indices)):\n",
                "        train_sampler = SubsetRandomSampler(train_idx)\n",
                "        val_sampler = SubsetRandomSampler(val_idx)\n",
                "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
                "        model.fc = nn.Linear(model.fc.in_features, 3)\n",
                "        model = model.to(device)    \n",
                "        loss_function = LabelSmoothingLoss()\n",
                "        optimizer = optim.AdamW(model.parameters(), lr=0.00020711569050274695,  weight_decay=0.000147)\n",
                "\n",
                "        train_fold_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
                "        val_fold_loader = DataLoader(dataset, batch_size=64, sampler=val_sampler)\n",
                "\n",
                "        val_accuracy = train_and_evaluate(\n",
                "            train_fold_loader, val_fold_loader,\n",
                "            model, optimizer, loss_function,\n",
                "            device, num_epochs=8\n",
                "        )\n",
                "        if val_accuracy > highest_acc:\n",
                "            best_model = model\n",
                "            \n",
                "\n",
                "        fold_accuracies.append(val_accuracy)\n",
                "\n",
                "        if trial:\n",
                "            trial.report(val_accuracy, fold)\n",
                "            if trial.should_prune():\n",
                "                raise optuna.TrialPruned()\n",
                "    torch.save(best_model, 'model2.pth')\n",
                "    return fold_accuracies\n",
                "\n",
                "\n",
                "def objective(trial):\n",
                "    k_folds = 5\n",
                "    dataset = train_loader.dataset\n",
                "    fold_accuracies = k_fold(dataset, k_folds,trial)\n",
                "    return np.mean(fold_accuracies)\n",
                "\n",
                "def run_optimization(n_trials=10):\n",
                "    study = optuna.create_study(\n",
                "        direction='maximize',\n",
                "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5)\n",
                "    )\n",
                "    study.optimize(\n",
                "        objective,\n",
                "        n_trials=n_trials,\n",
                "        timeout=3600  # 1 hour timeout\n",
                "    )\n",
                "    \n",
                "    print('\\nBest trial:')\n",
                "    print('Value:', study.best_value)\n",
                "    print('Params:', study.best_params)\n",
                "    \n",
                "    return study\n",
                "\n",
                "\n",
                "# study = run_optimization()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "ename": "RuntimeError",
                    "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet50\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResNet50_Weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# model = torch.load(\"model.pth\").to(device)\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
                        "File \u001b[0;32m/usr/pkg/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
                    ]
                }
            ],
            "source": [
                "num_epochs = 8\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).to(device)\n",
                "model.fc = nn.Linear(model.fc.in_features, 3).to(device)\n",
                "# model = torch.load(\"model.pth\").to(device)\n",
                "loss_function = LabelSmoothingLoss()\n",
                "optimizer = optim.AdamW(model.parameters(), lr=0.00020711569050274695, weight_decay=0.000147)\n",
                "\n",
                "\n",
                "model = torch.load(\"model.pth\").to(device)\n",
                "model2 = torch.load(\"model2.pth\").to(device)\n",
                "\n",
                "result1= evaluate(model,test_loader,device)\n",
                "result2 = evaluate(model2,test_loader, device)\n",
                "\n",
                "print(result1)\n",
                "print(result2)\n",
                "    \n",
                "# result = train_and_evaluate(train_loader, test_loader, model, optimizer, loss_function, device, num_epochs)\n",
                "# result = k_fold(train_loader.dataset, 5)\n",
                "\n",
                "\n",
                "# print(result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'result' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m)\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
                    ]
                }
            ],
            "source": [
                "print(result)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
